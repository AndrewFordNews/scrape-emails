{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (1.2.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for re\u001b[0m\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests) (3.0.4)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for urllib\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for collections\u001b[0m\n",
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for time\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's get set up, make sure everything is installed and imported \n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install joblib\n",
    "!{sys.executable} -m pip install re\n",
    "!{sys.executable} -m pip install requests\n",
    "!{sys.executable} -m pip install urllib\n",
    "!{sys.executable} -m pip install collections\n",
    "!{sys.executable} -m pip install bs4\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install time\n",
    "\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a string of text that includes the URLs you want to search for email addresses\n",
    "\n",
    "text = '''TEXT INCLUDING www.URLS.com HERE'''\n",
    "\n",
    "# for example, this should look like: \n",
    "\n",
    "# text = '''SUMMARY OF CITY & TOWN INFORMATION\n",
    "# APACHE JUNCTION, CITY OF\n",
    "# 300 East Superstition Boulevard\n",
    "# Apache Junction, AZ 85119\n",
    "# Phone: (480) 982-8002\n",
    "# Fax: 0\n",
    "# www.apachejunctionaz.gov\n",
    "# AVONDALE, CITY OF\n",
    "# 11465 West Civic Center Drive\n",
    "# Avondale, AZ 85323\n",
    "# Phone: (623) 333-1000\n",
    "# Fax: (623) 333-0101\n",
    "# www.avondaleaz.gov\n",
    "# BENSON, CITY OF\n",
    "# P.O. Box 2223\n",
    "# Benson, AZ 85602\n",
    "# Phone: (520) 586-2245\n",
    "# Fax: (520) 586-3375\n",
    "# www.cityofbenson.com'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of URL(s) you're about to scrape: 3\n"
     ]
    }
   ],
   "source": [
    "urls = list(re.findall(URL_REGEX, text))\n",
    "\n",
    "print(\"count of URL(s) you're about to scrape: \"+str(len(urls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign integer for the depth of how many pages you want to scrape on each URL\n",
    "\n",
    "limit_urls_to = 100\n",
    "\n",
    "# assign an integer for how many emails you want to reach before moving to the next URL\n",
    "\n",
    "limit_emails_to = 20\n",
    "\n",
    "# the code will move to the next URL when it hits the first of the above thresholds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much of this is lifted from https://pythoncircle.com/post/217/python-script-2-crawling-all-emails-from-a-website/\n",
    "\n",
    "def scrape_sites(target_url):\n",
    "\n",
    "    start = time.time()  \n",
    "\n",
    "    starting_url = 'http://'+target_url\n",
    "\n",
    "    # a queue of urls to be crawled\n",
    "    unprocessed_urls = deque([starting_url])\n",
    "\n",
    "    # set of already crawled urls\n",
    "    processed_urls = set()\n",
    "\n",
    "    # a set of fetched emails\n",
    "    emails = set()\n",
    "\n",
    "    # setting the limit of how many pages you're going to scrape per each base URL, before moving on\n",
    "    url_limit = limit_urls_to\n",
    "\n",
    "    # setting the limit of how many emails you're going to gather per each base URL, before moving on\n",
    "    email_floor = limit_emails_to\n",
    "\n",
    "    # process urls one by one from unprocessed_url queue until queue is empty\n",
    "    while len(unprocessed_urls) and (sum((itm.count(target_url) for itm in processed_urls)))<url_limit and len(emails)<=email_floor:\n",
    "\n",
    "        # move next url from the queue to the set of processed urls\n",
    "        url = unprocessed_urls.popleft()\n",
    "        processed_urls.add(url)\n",
    "\n",
    "        # extract base url to resolve relative links\n",
    "        parts = urlsplit(url)\n",
    "        base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "        path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "        # get url's content\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Crawling URL %s\" % url)\n",
    "        print(str(sum((itm.count(target_url) for itm in processed_urls)))+ \" URLs crawled in \" + target_url + \" in \" + str(elapsed) +\" seconds \")\n",
    "        print(\"So that's \" + (str(elapsed/(sum((itm.count(target_url) for itm in processed_urls))))) + \" seconds per page\")\n",
    "        print(\"and \" + str(len(emails)) + \" emails\")\n",
    "        print(\"\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            # ignore pages with errors and continue with next url\n",
    "            pass\n",
    "        \n",
    "\n",
    "        # extract all email addresses and add them into the resulting set\n",
    "        # You may edit the regular expression as per your requirement\n",
    "        new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I))\n",
    "        emails.update(new_emails)\n",
    "        #print(emails)\n",
    "\n",
    "        # create a beutiful soup for the html document\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Once this document is parsed and processed, now find and process all the anchors i.e. linked urls in this document\n",
    "        for anchor in soup.find_all(\"a\"):\n",
    "\n",
    "            # extract link url from the anchor\n",
    "            link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "\n",
    "            # resolve relative links (starting with /)\n",
    "            if link.startswith('/'):\n",
    "                link = base_url + link\n",
    "            elif not link.startswith('http'):\n",
    "                link = path + link\n",
    "\n",
    "            # add the new url to the queue if it was not in unprocessed list nor in processed list yet\n",
    "            if not link in unprocessed_urls and not link in processed_urls and url in link: \n",
    "                unprocessed_urls.append(link)\n",
    "\n",
    "        output = pd.DataFrame(list(emails))\n",
    "    \n",
    "    return output.to_csv(target_url+'_emails_.csv')\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  12\n"
     ]
    }
   ],
   "source": [
    "# spin up your computer's full horsepower to speed up this process \n",
    "\n",
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy all your processors to scraping URLs\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "try: \n",
    "    Parallel(n_jobs=num_cores)(delayed(scrape_sites)(target_url) for target_url in urls)\n",
    "\n",
    "except: \n",
    "    pass \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
